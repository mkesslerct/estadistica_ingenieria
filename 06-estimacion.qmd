# Introducción a la teoría de la estimación

## Introducción

Consideramos un experimento aleatorio para el cual estamos dispuestos a
escoger un modelo, posiblemente con uno o varios parámetros que
tendremos que ajustar. Por ejemplo, queremos realizar una medición con
un aparato, la variable que nos interesa es $X$ "valor proporcionado por
el aparato", pensamos que la distribución de los valores que puede tomar
$X$ se puede aproximar por una distribución Normal. Nos falta "ajustar"
los valores de la media y de la varianza de esta distribución normal,
para disponer de un modelo completamente especificado que nos permitirá
realizar cálculos de probabilidad, predicciones etc\... Para ajustar los
parámetros que nos faltan, repetiremos el experimento varias veces y
sacaremos información - se dice inferir - sobre estos parámetros a
partir de los valores obtenidos de $X$. El primer tipo de información
que podemos intentar sacar es acerca de su valor. Estimar un parámetro
consiste en obtener una aproximación de su valor en base a los datos de
la variable correspondientes a varias realizaciones del experimento.
Recordar que vimos en el tema anterior que los datos provenientes de
varias realizaciones del experimento constituyen una **muestra** de la
distribución de $X$.

## Estimación puntual

### Definición

Consideramos un experimento aleatorio, con una v.a $X$, y un modelo para
la distribución de $X$. Este modelo incluye parámetros desconocidos.
Disponemos de una muestra de la distribución de $X$.

::: {#def-estadistico}
Cualquier estadístico (es decir, cualquier función de las observaciones
de la muestra) diseñado para aproximar el valor de un parámetro $\theta$
del modelo, se llama estimador puntual del parámetro $\theta$.
:::

En la tabla siguiente se presentan algunos parámetros usuales y los
estimadores asociados:

::: center
  $\theta$                Estimador
  ------------ --------------------------------
  $\mu$           $\bar{X}$, media muestral
  $\sigma^2$       $S^2$, varianza muestral
  $p$           $\hat{p}$, proporción muestral
:::

Un aspecto fundamental de un estimador es que es una variable aleatoria:
su valor concreto depende de la muestra escogida. Utilizaremos los
resultados del tema anterior sobre distribuciones muestrales para
deducir propiedades de las distribuciones de los estimadores más usados.

### Propiedades deseables para un estimador

#### Estimador insesgado

Una primera propiedad deseable para un estimador es que el centro de la
distribución de los valores que puede tomar coincida con el valor del
parámetro que queremos aproximar. Si éste es el caso, decimos que el
estimador es insesgado. Así, si $\hat{\theta}$ es un estimador del
parámetro $\theta$, decimos que $\hat{\theta}$ es un estimador insesgado
de $\theta$ si $$\mathbb{E}[\hat{\theta}]=\theta.$$

Comprobemos si los estimadores más usados son insesgados:

-   La media muestral $\bar{X}$: hemos visto en el tema 5 que, sea cual
    sea la distribución de $X$, se cumple que $\mathbb{E}[\bar{X}]=\mu_X$.
    Deducimos que $\bar{X}$ es un estimador insesgado de $\mu_X$.

-   La varianza muestral $S^2$. Tenemos que
    $$S^2=\frac n {n-1} [\overline{X^2}-(\bar{X})^2].$$ Por lo tanto,
    $$\mathbb{E}[S^2]=\frac n {n-1} [\mathbb{E}[\overline{X^2}]-\mathbb{E}[(\bar{X})^2]].$$
    Necesitamos calcular por una parte $\mathbb{E}[\overline{X^2}]$ y por otra
    parte $\mathbb{E}[(\bar{X})^2]$. Al ser $\overline{X^2}$ la media muestral
    de la variable $X^2$, sabemos por el tema 5 que
    $\mathbb{E}[\overline{X^2}=\mathbb{E}[X^2]=var(X)+\mu_X^2.$. Por otra parte,
    $\mathbb{E}[(\bar{X})^2]]=var(\bar{X})+(\mathbb{E}[\bar{X}])^2=\frac {\sigma^2} n + \mu_X^2$.
    Deducimos que
    $$\mathbb{E}[S^2]=\frac n {n-1} [\sigma^2-\frac{ \sigma^2} n ]=\sigma^2.$$
    Hemos por lo tanto comprobado que la varianza muestral es un
    estimador insesgado de la varianza. De hecho, este resultado
    constituye la justificación de que la varianza muestral se defina
    con el factor $n/(n-1)$, para que el estimador resulte insesgado.

-   Proporción muestral $\hat{p}$: en el tema 5, hemos obtenido la
    caracterización de $\hat{p}$ como $N/ n$ donde $N$ es el número de
    elementos en la muestra con la característica de interés, y hemos
    visto que $N\sim\mathcal{B}(n,p)$. Deducimos que
    $$\mathbb{E}[\hat{p}]=\frac {\mathbb{E}[N]}{n}= \frac {np}n=p.$$ En este caso
    también, la proporción muestral resulta ser un estimador insesgado
    de la proporción.

#### Estimador consistente

Si un estimador es insesgado, nos interesa que la dispersión de los
valores que puede tomar sea la más pequeña posible, para que la
precisión de la estimación sea la mayor posible. Por consiguiente, una
buena propiedad adicional de un estimador insesgado es que su varianza
tienda a cero si el número de observaciones $n$ crece hacia infinito. En
este caso, se dice que el estimador es consistente.

De la misma manera que en el apartado anterior, podemos deducir,
utilizando los resultados del tema 5, que
$$var(\bar{X})=\frac {\sigma^2} n, \quad var(\hat{p})=var(\frac N n)= \frac 1 {n^2} var(N)=\frac{p(1-p)} n.$$
Es fácil comprobar que, en efecto tanto $var(\bar{X})$ como
$var(\hat{p})$ tienden a cero si $n$ tiende a infinito, es decir que son
dos estimadores consistentes.

### Métodos de construcción de estimadores

En los ejemplos de las secciones anteriores, los estimadores propuestos
están basados en estadísticos naturales para los parámetros de interés:
la media muestral para estimar la media, la proporción muestral para
estimar la proporción, etc\... En modelos más sofisticados es útil
disponer de métodos generales de construcción de estimadores razonables.

#### Estimadores de momentos

Es el método más antiguo de construcción de estimadores y se debe a Karl
Pearson a principios del siglo XX.

Consideremos una v.a. $X$ y un modelo para la distribución de sus
valores, que consiste en la especificación de $x\mapsto f_X(x;\theta)$,
siendo $f_X$ la función puntual de probabilidad, o la función de
densidad según si $X$ es una variable discreta o continua.

El parámetro $\theta$ es posiblemente multidimensional, llamamos $p$ su
dimensión, es decir que $p$ es el número de parámetros desconocidos en
el modelo. Para un entero $k$, consideramos el momento $\mu_k$ de orden
$k$ de la distribución de $X$: $$\mu_k=\mathbb{E}[X^k].$$ Cabe destacar que la
expresión de $\mu_k$ depende del parámetro $\theta$. Para enfatizar esta
dependencia, escribiremos $\mu_k(\theta)$ para denotar el momento de
orden $k$ del modelo descrito por $x\mapsto f_X(x;\theta)$. De manera
paralela, definimos el momento *muestral* de orden $k$:
$$m_k=\overline{X^k}=\frac {X_1^k+\ldots+X_n^k} n.$$ Para un parámetro
de dimensión $p$, los estimadores de los momentos se obtienen igualando
los $p$ primeros momentos del modelo para la distribución de $X$ con sus
equivalentes muestrales: 

\begin{align}
\mu_1(\theta)&=&\overline{X},\\
\mu_2(\theta)&=&\overline{X^2},\\
\vdots&=&\vdots,\\
\mu_k(\theta)&=&\overline{X^k}.
\end{align}

 Calculemos para ilustrar el método los estimadores de
momentos en los modelos siguientes:

-   $X\sim{\cal N}(\mu,\sigma^2).$, donde $\theta=(\mu,\sigma^2)$.
    Necesitamos igualar los dos primeros momentos con sus equivalentes
    muestrales. Los dos primeros momentos de la distribución
    ${\cal N}(\mu,\sigma^2)$ son 

\begin{align}
    \mu_1(\theta)&=&\mu\\
    \mu_2(\theta)&=&\mathbb{E}[X^2]=Var(X)+(\mathbb{E}[X])^2=\sigma^2+\mu^2.\\
\end{align}

 Deducimos que los estimadores de los momentos son
    solución del sistema: 

\begin{align}
    \mu&=&\overline{X}\\
    \sigma^2+\mu^2&=&\overline{X^2},\\
\end{align}

 es decir
    $$\hat{\mu}=\overline{X},\quad \hat{\sigma^2}=\overline{X^2}-(\overline{X})^2.$$

-   Modelo de Bernoulli: $X\sim { Bernoulli}(p)$, donde desconocemos
    $p$. Sólo necesitamos igualar el primer momento con su equivalente
    muestral, obtenemos $$\hat{p}=\bar{X},$$ puesto que $X_1,\ldots,X_n$
    sólo pueden tomar el valor 1 o el valor 0, su media es igual a la
    proporción muestral de 1. El estimador de momentos de la proporción
    $p$ en un modelo de Bernoulli es la proporción muestral.

#### Método de máxima verosimilitud

El método de máxima verosimilitud es sin dudas el método más utilizado
de construcción de un estimador puntual.

##### Verosimilitud

Sea $X$ una v.a, con distribución especificada por
$x\mapsto f_X(x;\theta)$, donde $\theta$ es el vector de parámetros, de
dimensión $p$. Repetimos el experimento $n$ veces y consideramos la
muestra aleatoria simple de la distribución de $X$: $(X_1,\ldots,X_n)$.
La distribución de la v.a $n$-dimensional $(X_1,\ldots,X_n)$ está
descrita a través de la relación
$$f_{X_1,\ldots,X_n}(x_1,\ldots,x_n;\theta)=f_{X_1}(x_1,\theta)\ldots f_{X_n}(x_n,\theta),$$
puesto que las v.a $X_1,\ldots,X_n$ son independientes. En esta última
igualdad, $f$ representa o bien la función puntual de probabilidad o
bien la función de densidad.

Para un valor concreto de $(X_1,\ldots,X_n)$, que denotamos por
$(x_1,\ldots,x_n)$, consideramos la función de $\theta$:
$$L_n:\left\{\begin{array}{l}
\mathbb{R}^p\to \mathbb{R}\\
\theta\mapsto L_n(\theta)=f_{X_1,\ldots,X_n}(x_1,\ldots,x_n;\theta).
\end{array}\right.$$ La función $L_n$ asocia a cada valor de $\theta$ el
valor de la densidad (o de la función puntual de probabilidad) de las
observaciones $(X_1,\ldots,X_n)$ evaluada en $(x_1,\ldots,x_n)$, los
valores concretos observados.

Consideremos la tirada de una moneda y asociamos la v.a. $X$ que valga 1
si sale cara y 0 si sale cruz. Utilizamos un modelo de Bernoulli de
parámetro $p$ entre 0 y 1. Tiramos 10 veces la moneda y obtenemos la
secuencia de valores siguiente: 0, 0, 1, 0, 1, 1, 1, 1, 1, 1. La
verosimilitud asocia a cada valor posible de $p$, la cantidad
$$\mathbb{P}(X_1=0;\ X_2=0;\ X_3=1;\ X_4=0;\ X_5=1;\ X_6=1;\ X_7=1;\ X_8=1;\ X_9=1;\ X_{10}=1).$$
Deducimos que $L_n(p)=(1-p)(1-p)p(1-p)(1-p)^6=(1-p)^3\cdot p^7.$ Se
representa la gráfica de la función $L_n(p)$ en la 
@fig-verosimilitud 


```{python}
#| label: fig-verosimilitud
#| fig-cap: "Verosimilitud correspondiente al ejemplo de 10 tiradas de una moneda."

import numpy as np
import matplotlib.pyplot as plt
p = np.linspace(0, 1, num=1001)
n = 10
n1 = 7
l = (1 - p)**(n - n1) * p**n1

fig, ax = plt.subplots()
ax.plot(p, l)
ax.set_xlabel("p")
ax.set_ylabel("Verosimilitud")
plt.show()
```

La verosimilitud nos indica para qué valor de $p$, la probabilidad de
haber observado la secuencia 0, 0, 1, 0, 1, 1, 1, 1, 1, 1 es la más
alta.

##### Estimador de máxima verosimilitud

::: definicion
Dados $(x_1,\ldots,x_n)$ los valores observados de una muestra,
consideramos la verosimilitud $\theta\mapsto L_n(\theta)$.

El estimador de máxima verosimilitud $\hat{\theta}$ de $\theta$ es
cualquier valor de $\theta$ que maximiza $\theta\mapsto L_n(\theta)$,
$$\hat{\theta}=\mathop{argmax}_{\theta} L_n(\theta).$$ La
maximización se realiza sobre todos los valores admisibles para el
parámetro $\theta$.
:::

Consideramos $X\sim{Bernoulli(p)}$. Observamos $x_1,\ldots,x_n$ una
realización de la muestra aleatoria simple $(X_1,\ldots,X_n)$. Puesto
que si $x=0,\ 1$, $f_X(x)=\mathbb{P}(X=x)=p^x\cdot(1-p)^{(1-x)}$, la
verosimilitud es
$$L_n(p)=p^{x_1}\cdot(1-p)^{(1-x_1)}\ldots p^{x_n}\cdot(1-p)^{(1-x_n)}=p^{\sum x_i}(1-p)^{n-\sum x_i}.$$
Los candidatos a alcanzar el máximo se obtienen derivando la
verosimilitud, o de manera equivalente y más sencilla, su logaritmo
(llamado log-verosimilitud):
$$\frac{d\log L_n}{dp}(p)=(n-\sum x_i)\left(-\frac 1 {1-p}\right)+\frac{\sum x_i} p =0.$$
Despejamos $p$ y encontramos $\hat{p}=(\sum x_i)/n$. Comprobamos además
que la derivada segunda de $L_n$ es negativa, lo que implica que
$\hat{p}$ es efectivamente un máximo global. Deducimos que el estimador
de máxima verosimilitud de $p$ es la proporción muestral.

Consideramos $X\sim{\cal N}(\mu,\sigma^2)$. Observamos $x_1,\ldots,x_n$
una realización de la muestra aleatoria simple $(X_1,\ldots,X_n)$. La
verosimilitud se obtiene a partir de la expresión de la densidad de $X$:
$$L_n(\mu,\sigma^2)=\prod_{i=1}^n \frac 1 {\sqrt{2\pi\sigma^2}} e^{-\frac {(x_i-\mu)^2}{2\sigma^2}}=\frac 1  {(2\pi\sigma^2)^{n/2}} e^{-\frac {\sum_{i=1}^n (x_i-\mu)^2}{2\sigma^2}}.$$
La log-verosimilitud es
$$\log L_n(\mu,\sigma^2)=-\frac n 2 \log(2\pi\sigma^2)-\frac {\sum_{i=1}^n (x_i-\mu)^2}{2\sigma^2}.$$
Para encontrar el máximo, calculamos las derivadas parciales de
$\log L_n$ respeto de $\mu$ y $\sigma^2$: 

\begin{align}
\frac{\partial}{\partial \mu}\log L_n(\theta)&=&\frac {\sum_{i=1}^n (x_i-\mu)^2}{\sigma^2}\\
\frac{\partial}{\partial \sigma^2}\log L_n(\theta)&=&-\frac n 2 \frac 1 {\sigma^2} + \frac {\sum_{i=1}^n (x_i-\mu)^2}{2(\sigma^2)^2}.
\end{align}

 Resolvemos $\frac{\partial}{\partial \mu}L_n=0$ y
$\frac{\partial}{\partial \sigma^2}L_n=0$, y encontramos que los dos
candidatos a máximo son
$$\hat{\mu}=\frac{\sum_{i=1}^n x_i} n,\quad \widehat{\sigma^2}=\frac{\sum_{i=1}^n (x_i-\hat{\mu})^2} n= \frac n {n-1} s^2.$$
Para comprobar que son efectivamente máximos globales, podemos fijarnos
en la expresión de la log-verosimilitud:
$$\log L_n(\mu,\sigma^2)=-\frac n 2 \log(2\pi\sigma^2)-\frac {\sum_{i=1}^n (x_i-\mu)^2}{2\sigma^2}.$$
Sea cual sea el valor de $\sigma^2$, la función
$\mu\mapsto \log L_n(\mu,\sigma^2)$ alcanza su máximo cuando
$\sum_{i=1}^n(x_i-\mu)$ es mínimo, es decir cuando
$\mu=(\sum_{i=1}^n x_i)/n$. El máximo de
$(\mu,\sigma^2)\mapsto \log L_n(\mu,\sigma^2)$ corresponderá por lo
tanto al máximo de la función
$\sigma^2\mapsto \log L_n(\hat{\mu},\sigma^2)$. Es fácil comprobar que
$\sigma^2\mapsto \log L_n(\hat{\mu},\sigma^2)$ alcanza su máximo en
$\widehat{\sigma^2}=\frac{\sum_{i=1}^n (x_i-\hat{\mu})^2} n= \frac n {n-1} s^2$.

Los estimadores de máxima verosimilitud de $\mu$ y $\sigma^2$ son por lo
tanto la media muestral y la llamada varianza muestral sesgada
$\widehat{\sigma^2}=\frac{\sum_{i=1}^n (x_i-\hat{\mu})^2} n= \frac n {n-1} s^2.$
En un apartado anterior hemos visto como la varianza muestral $s^2$ es
un estimador insesgado, por lo tanto
$\mathbb{E}[\widehat{\sigma^2}]=\frac {n-1} n \sigma^2.$ Es un ejemplo en él que
el método de máxima verosimilitud proporciona un estimador sesgado.

## Estimación por intervalos

No queremos limitarnos a dar un valor para aproximar un parámetro sino
proporcionar también una medida del error que pensamos cometer. Para
ello, calcularemos un intervalo en él que pensamos que se encuentra el
parámetro.

### Idea básica

Supongamos que queremos estimar la media $\mu$ de una v.a. $X$ cuya
distribución es Normal con una desviación típica igual a 2 unidades, es
decir $X\sim \mathcal{N}(\mu,4)$. Para ello, extraigo una muestra de
tamaño 4, y estimo $\mu$ por el valor de $\bar{X}$. Por el tema 5, ver
@sec-xbarnormal, sabemos que la distribución de $\bar{X}$ es
$\mathcal{N}(\mu,\sigma^2/n)$ es decir $\mathcal{N}(\mu,1)$. Por la
propiedad de la distribución Normal, ver
@sec-propiedadesnormal, deducimos que el 95% de las muestras
proporcionan un valor de $\bar{X}$ que se encuentra a menos de 2
unidades de la media $\mu$.

Invertamos ahora la situación: sé donde está $\bar{X}$, ¿donde está
$\mu$? Por la misma regla, se encuentra, para el 95% de las muestras, a
menos de 2 unidades de $\bar{X}$, es decir que $\mu$ se encuentra en el
intervalo $[\bar{X}-2,\bar{X}+2]$. Dicho de otra manera, para el 95% de
las muestras, el intervalo aleatorio $[\bar{X}-2,\bar{X}+2]$ captura el
valor del parámetro $\mu$.

### Intervalo de confianza para la media $\mu$ de una distribución Normal con varianza conocida

#### Construcción

Consideramos la variable $X\sim\mathcal{N}(\mu,\sigma^2)$. Suponemos que
conocemos el valor de $\sigma^2$. La construcción del intervalo de
confianza para la media $\mu$ se realiza siguiendo los siguientes pasos.

-   Nos fijamos el llamado "nivel de riesgo", $\alpha$ un número entre 0
    y 1. La cantidad $1-\alpha$ expresada en porcentaje se llama **nivel
    de confianza**.\
    Los valores más utilizados de $\alpha$ son $0.1$, $0.05$, y $0.01$,
    lo que corresponde con niveles de confianza del $90\%$ ,$95\%$ y
    $99\%$ respectivamente.

-   Escogemos el estadístico $\bar{X}$ para estimar $\mu$. Su
    distribución en su forma tipificada es
    $$\frac {\bar{X}-\mu} {\sigma/\sqrt{n}}\sim \mathcal{N}(0,1).$$ Para
    $0\leq u\leq 1$, utilizamos la notación $z_u$ para denotar el
    cuantil $u$ de la distribución Normal estándar, es decir el valor
    que cumple $\mathbb{P}(Z\leq z_u)=u$, o dicho de otra manera, el valor que
    deja a su izquierda un área igual a $u$ debajo de la curva de la
    densidad Normal estándar. En particular usaremos de manera repetida
    los cuantiles siguientes: $z_{0.95}$, $z_{0.975}$ y $z_{0.995}$.
    Para conocer sus valores, podemos buscar en la tabla de la Normal
    estándar, los valores $0.95$, $0.975$ y $0.995$ en la columna de las
    probabilidades $\phi(t)$ y apuntar los valores correspondientes de
    $t$. Encontramos $z_{0.95}=1.64$, $z_{0.975}=1.96$ y
    $z_{0.995}=2.56$.

-   Dibujo en la densidad del estadístico
    $\frac {\bar{X}-\mu} {\sigma/\sqrt{n}}$, una región central que
    represente el $100(1-\alpha)\%$ del área total, tal como viene
    ilustrado en la @fig-densidad-estadistico-Z.

```{python}
#| label: fig-densidad-estadistico-Z
#| fig-cap: Región central con un área $1 - \alpha$ en la densidad de una Normal estándar. 
#| warning: false
#| fig-width: 10
#| fig-height: 7
from scipy.stats import norm
from matplotlib import pyplot as plt
import numpy as np 
alpha = 0.05
fig, ax = plt.subplots(1, 1)
x = np.linspace(norm.ppf(0.001), norm.ppf(0.999), 100)
interval = (x <= norm.ppf(1 - alpha / 2)) & (x >= norm.ppf(alpha / 2))
ax.plot(x, norm.pdf(x))
ax.fill_between(x, norm.pdf(x), alpha=0.4, hatch='//', where=interval)
ax.get_yaxis().set_visible(False)
ax.set_xlabel('')
ax.set_xticks([norm.ppf(alpha / 2), 0, norm.ppf(1 - alpha / 2)])
ax.set_xticklabels([r'$-z_{1-\alpha/2}$', '0', r'$z_{1-\alpha/2}$'])
plt.show()
```

-   Deducimos
    $$\mathbb{P}(-z_{1-\alpha/2}\leq \frac {\bar{X}-\mu} {\sigma/\sqrt{n}}\leq z_{1-\alpha/2})=1-\alpha.$$
    Despejamos $\mu$ en las desigualdades 

\begin{align}
    \mathbb{P}(-z_{1-\alpha/2}{\sigma/\sqrt{n}}\leq  {\bar{X}-\mu} \leq z_{1-\alpha/2}{\sigma/\sqrt{n}})&=&1-\alpha\\
    \Leftrightarrow \mathbb{P}(-\bar{X}-z_{1-\alpha/2}{\sigma/\sqrt{n}}\leq -\mu \leq -\bar{X}+ z_{1-\alpha/2}{\sigma/\sqrt{n}})&=&1-\alpha\\
    \Leftrightarrow \mathbb{P}(\bar{X}+z_{1-\alpha/2}{\sigma/\sqrt{n}}\geq \mu \geq \bar{X}- z_{1-\alpha/2}{\sigma/\sqrt{n}})&=&1-\alpha\\
    \Leftrightarrow \mathbb{P}(\bar{X}-z_{1-\alpha/2}{\sigma/\sqrt{n}}\leq \mu \leq \bar{X}+ z_{1-\alpha/2}{\sigma/\sqrt{n}})&=&1-\alpha
\end{align}

-   El intervalo de confianza al $100(1-\alpha)\%$ para $\mu$ es
    $$\mu \in [\bar{X}-z_{1-\alpha/2}{\sigma/\sqrt{n}}; \bar{X}+ z_{1-\alpha/2}\sigma/\sqrt{n}].$$
    Se escribe también de otra manera equivalente:
    $$\mu = \bar{X}\pm z_{1-\alpha/2}{\sigma/\sqrt{n}},$$ el término
    $z_{1-\alpha/2}{\sigma/\sqrt{n}}$ se llama término de error.

#### Interpretación

El intervalo
$[\bar{X}-z_{1-\alpha/2}{\sigma/\sqrt{n}}; \bar{X}+ z_{1-\alpha/2}\sigma/\sqrt{n}]$
es un intervalo aleatorio, puesto que sus extremos dependen de la
muestra escogida. Por su construcción, sabemos que este intervalo
aleatorio tiene una probabilidad de $100(1-\alpha)\%$ de capturar el
valor de $\mu$. Es decir que, al extraer una muestra, tengo una
probabilidad igual a $1-\alpha$ de que el intervalo que calcularé
efectivamente capture el valor $\mu$ que busco. También tengo una
probabilidad $\alpha$ de que, al afirmar que $\mu$ se encuentra en
$[\bar{X}-z_{1-\alpha/2}{\sigma/\sqrt{n}}; \bar{X}+ z_{1-\alpha/2}\sigma/\sqrt{n}]$,
me equivoque. Sin embargo, esta probabilidad $\alpha$, el riesgo de
equivocarme, se fija en general bastante pequeño, por ejemplo
$\alpha=0.05$.

Para ilustrar esta interpretación, se han simulado 20 veces el proceso de
extraer al azar una muestra de tamaño 4 de una distribución
$X\sim \mathcal{N}(\mu_X,1)$. Se representan en la  @fig-intervalos en el eje Ox
el número de la muestra y en el eje Oy el intervalo de confianza
asociado. Además una línea horizontal representa el valor de $\mu$ que
se pretende estimar, en este caso $\mu=2$. La gran mayoría de los
intervalos capturan el valor correcto de $\mu$, pero hay un intervalo,
el correspondiente a la muestra número 7 que no lo hace: este intervalo
es erróneo, y esta muestra forma parte del 5% de las muestras "malas",
es decir las que proporcionan intervalos equivocados.


```{python}
#| label: fig-intervalos
#| fig-cap: Los intervalos de confianza al 95% correspondientes a 20 muestras de tamaño 4. La media que se busca estimar es $\mu=2$.
#| warning: false
#| fig-width: 10
#| fig-height: 7
import numpy as np
from numpy.random import default_rng
from matplotlib import pyplot as plt

rng = default_rng(31415)
m = 20
n = 4
mu = 2
sigma = 1
samples = 1 + np.arange(m)
means = rng.normal(mu, sigma/np.sqrt(n), size=m) 
error = 1.96 * sigma / np.sqrt(n)
fig, ax = plt.subplots()
ax.set_xlim(0, m + 1)
ax.set_ylim(np.min(means) - 1.5 * error, np.max(means) + 1.5 * error) 
ax.axhline(mu, 0, m + 1, color='r', linestyle='dashed')
ax.errorbar(samples, means, yerr=error, fmt='bo', markersize=8, capsize=2)
ax.set_xlabel('Muestra')
ax.set_ylabel('Valores')
ax.set_xticklabels([])
ax.set_xticks(np.arange(1, 21))
plt.show()
```
#### Ejemplo

Supongamos que queremos estimar la longitud media de un artículo
producido por una máquina. Por experiencia, sabemos que es razonable
modelizar la distribución de los valores de la longitud de los artículos
producidos por una distribución Normal con media $\mu$ y desviación
típica igual a 0.05. Para estimar $\mu$ extraemos una muestra de 5
artículos y construimos un intervalo de confianza al 90%. Supongamos que
los datos que se obtienen son los siguientes:

::: center
20.1, 20.05, 20.01, 19.95, 19.99.
:::

El intervalo de confianza es
$\mu \in [\bar{X}-z_{1-\alpha/2}\sigma/\sqrt{n}, \bar{X}+z_{1-\alpha/2}\sigma/\sqrt{n}]$.
Necesitamos $\bar{X}$, es fácil comprobar que $\bar{X}=20.02$, por otra
parte, al haber escogido 90% de confianza, fijamos $\alpha=0.1$.
Deducimos de la tabla Normal que $z_{1-\alpha/2}=z_{0.95}=1.64$.
Concluimos que el intervalo buscado será
$$[20.02-1.64\frac {0.05} {\sqrt{5}},20.02+1.64\frac {0.05} {\sqrt{5}}],$$
es decir $\mu\in [19.98,20.06]$, o de forma equivalente
$\mu=20.02\pm 0.04$.

### Comentarios importantes

#### Si $X$ no es Normal

La construcción del intervalo de confianza está basada en la
hipótesis de que la distribución de la v.a. $X$ es Normal, puesto
que utilizamos
$$\frac {\bar{X}-\mu} {\sigma/\sqrt{n}}\sim \mathcal{N}(0,1).$$ Si
la distribución de $X$ no es Normal, el intervalo no es válido, es
decir que no podemos garantizar que la confianza especificada sea
cierta. Sin embargo, en el caso en que la muestra es grande, podemos
recurrir al Teorema Central del Límite, ver @thm-tcl y sabemos que
$$\frac {\bar{X}-\mu} {\sigma/\sqrt{n}}\sim \mathcal{N}(0,1),\mbox{ aproximadamente}$$
lo que posibilita que los intervalos sean aproximadamente válidos:
la confianza especificada no será exacta pero casi\...

¿A partir de cuantas observaciones consideramos una muestra como
grande? No hay respuesta universal, depende mucho de lo alejado que
está la distribución de $X$ de una distribución Normal. En general,
se suele considerar en práctica que $n\geq 30$ es suficiente para
que los intervalos construidos sean aproximadamente válidos.

#### Factores que afectan a la precisión de la estimación.

Recordar que en la estimación por un intervalo, el margen de error
es $\pm z_{1.\alpha/2}\sigma/\sqrt{n}$. Deducimos en particular que

-   cuanto mayor sea $n$, más precisa será la estimación, es decir
    que más pequeño será el intervalo de confianza.

-   cuanto menor sea $\sigma$, mayor precisión en la estimación.

-   cuanto mayor sea la confianza, peor será la precisión de la
    estimación: si queremos garantizar con gran confianza que el
    intervalo proporcionado captura $\mu$, no hay más remedio que
    proporcionar un intervalo grande\...

####  Si $\sigma$ es desconocida 
La hipótesis de que $\sigma$ es conocida no es realista: en general
también hay que estimarla a partir de la muestra. La distribución
del estadístico que resulta de sustituir $\sigma$ por $S$, la
desviación típica muestral, $\frac {\bar{X}-\mu} {S/\sqrt{n}}$ es
una t de Student con $n-1$ grados de libertad. Podemos repetir los
pasos de construcción del intervalo de confianza para $\mu$
basándonos en el estadístico $\frac {\bar{X}-\mu} {S/\sqrt{n}}$:

-   Nos fijamos el "nivel de riesgo", $\alpha$.\

-   Escogemos el estadístico
    $$T=\frac{\bar{X}-\mu}{S/\sqrt{n}}\sim t_{n-1}$$

-   Dibujo en la densidad del estadístico $T$ una región central que
    represente el $100(1-\alpha)\%$ del área total, tal como viene
    ilustrado en la @fig-densidad-estadistico-T.

```{python}
#| label: fig-densidad-estadistico-T
#| fig-cap: Región central con un área $1 - \alpha$ en la densidad de una t de Student. 
#| warning: false
#| fig-width: 10
#| fig-height: 7
from scipy.stats import t
from matplotlib import pyplot as plt
import numpy as np 
alpha = 0.05
df = 10
fig, ax = plt.subplots(1, 1)
x = np.linspace(t.ppf(0.001, df), t.ppf(0.999, df), 100)
interval = (x <= t.ppf(1 - alpha / 2, df)) & (x >= t.ppf(alpha / 2, df))
ax.plot(x, t.pdf(x, df))
ax.fill_between(x, t.pdf(x, df), alpha=0.4, hatch='//', where=interval)
ax.get_yaxis().set_visible(False)
ax.set_xlabel('')
ax.set_xticks([t.ppf(alpha / 2, df), 0, t.ppf(1 - alpha / 2, df)])
ax.set_xticklabels([r'$-t_{n-1, 1-\alpha/2}$', '0', r'$t_{n-1, 1-\alpha/2}$'])
plt.show()
```

-   Deducimos
$$\mathbb{P}(-t_{n-1,1-\alpha/2}\leq \frac {\bar{X}-\mu} {S/\sqrt{n}}\leq t_{n-1,1-\alpha/2})=1-\alpha,$$
donde hemos utilizado la notación $t_{n-1,1-\alpha/2}$ para
denotar el cuantil $1-\alpha/2$ de la distribución $t_{n-1}$, es
decir el punto que deja un área igual a $1-\alpha/2$ a su
izquierda. Los valores de los cuantiles más usados de la
distribución t están recogidos en una tabla en el apéndice de
este capítulo.

Despejamos $\mu$ en las desigualdades y obtenemos
$$\mathbb{P}(\bar{X}-t_{n-1,1-\alpha/2}{S/\sqrt{n}}\leq \mu \leq \bar{X}+ t_{n-1,1-\alpha/2}{S/\sqrt{n}})=1-\alpha.$$

-   El intervalo de confianza al $100(1-\alpha)\%$ para $\mu$ es
$$\mu \in [\bar{X}-t_{n-1,1-\alpha/2}{S/\sqrt{n}}; \bar{X}+ t_{n-1,1-\alpha/2}S/\sqrt{n}].$$
Se escribe también
$$\mu = \bar{X}\pm t_{n-1,1-\alpha/2}{S/\sqrt{n}},$$ el término
$t_{n-1,1-\alpha/2}{S/\sqrt{n}}$ es el término de error.

### Determinación del tamaño muestral

#### Planteamiento

Si estoy en condiciones de diseñar el experimento que quiero realizar
para estimar la media $\mu$, puedo intentar decidir del número de
observaciones en la muestra que serán necesarias para garantizar, con
una confianza dada, que el margen de error sea menor que una cantidad
prefijada. Es decir, que me fijo una cantidad $max$, y me pregunto cuál
deberá de ser el valor de $n$ para que
$$z_{1-\alpha/2}\frac \sigma{\sqrt{n}}\leq max.$$ Es fácil obtener $n$
despejándolo de la desigualdad.

#### Ejemplo

La medición de la conductividad de un material sigue una distribución
que modelizamos por una Normal con desviación típica $\sigma=0.5$.
Quiero construir un intervalo de confianza al 95% para el valor promedio
proporcionado de la conductividad pero quiero que el error cometido sea
menor de 0.3. ¿cuántas veces deberé repetir la medición?

Busco $n$ tal que $z_{1-\alpha/2}\sigma/\sqrt{n}\leq 0.3,$ sabiendo que
$\sigma=0.5$, y $\alpha=0.05$. Obtengo
$$1.96\frac {0.5} {\sqrt{n}}\leq 0.3,$$ es decir
que$$n\geq \left(\frac {1.96\cdot 0.5}{0.3}\right)^2\simeq 10.67.$$
Habrá por lo tanto que realizar 11 mediciones.

## Cuantiles de la distribución $t$ de Student {#cuantiles-de-la-distribución-t-de-student .unnumbered}

Valores de los cuantiles de la distribución $t$ de Student con $k$
grados de libertad: para un $0\leq p\leq 1$, el valor $t_{k,p}$
satisface $\mathbb{P}(t\leq t_{k,p})=p$.

::: center
  ------- --------------- -------------- --------------- -------------- -------------- -------------- -------------- -------------- -------------- --
    $k$    $t_{k,0.995}$   $t_{k,0.99}$   $t_{k,0.975}$   $t_{k,0.95}$   $t_{k,0.90}$   $t_{k,0.80}$   $t_{k,0.70}$   $t_{k,0.60}$   $t_{k,0.50}$  

     1        63,657          31,821         12,706          6,314          3,078          1,376          0,727          0,325          0,158      
     2         9,925          6,965           4,303           2,92          1,886          1,061          0,617          0,289          0,142      
     3         5,841          4,541           3,182          2,353          1,638          0,978          0,584          0,277          0,137      
     4         4,604          3,747           2,776          2,132          1,533          0,941          0,569          0,271          0,134      
     5         4,032          3,365           2,571          2,015          1,476           0,92          0,559          0,267          0,132      
     6         3,707          3,143           2,447          1,943           1,44          0,906          0,553          0,265          0,131      
     7         3,499          2,998           2,365          1,895          1,415          0,896          0,549          0,263           0,13      
     8         3,355          2,896           2,306           1,86          1,397          0,889          0,546          0,262           0,13      
     9         3,25           2,821           2,262          1,833          1,383          0,883          0,543          0,261          0,129      
    10         3,169          2,764           2,228          1,812          1,372          0,879          0,542           0,26          0,129      
    11         3,106          2,718           2,201          1,796          1,363          0,876           0,54           0,26          0,129      
    12         3,055          2,681           2,179          1,782          1,356          0,873          0,539          0,259          0,128      
    13         3,012           2,65           2,16           1,771           1,35           0,87          0,538          0,259          0,128      
    14         2,977          2,624           2,145          1,761          1,345          0,868          0,537          0,258          0,128      
    15         2,947          2,602           2,131          1,753          1,341          0,866          0,536          0,258          0,128      
    16         2,921          2,583           2,12           1,746          1,337          0,865          0,535          0,258          0,128      
    17         2,898          2,567           2,11            1,74          1,333          0,863          0,534          0,257          0,128      
    18         2,878          2,552           2,101          1,734           1,33          0,862          0,534          0,257          0,127      
    19         2,861          2,539           2,093          1,729          1,328          0,861          0,533          0,257          0,127      
    20         2,845          2,528           2,086          1,725          1,325           0,86          0,533          0,257          0,127      
    21         2,831          2,518           2,08           1,721          1,323          0,859          0,532          0,257          0,127      
    22         2,819          2,508           2,074          1,717          1,321          0,858          0,532          0,256          0,127      
    23         2,807           2,5            2,069          1,714          1,319          0,858          0,532          0,256          0,127      
    24         2,797          2,492           2,064          1,711          1,318          0,857          0,531          0,256          0,127      
    25         2,787          2,485           2,06           1,708          1,316          0,856          0,531          0,256          0,127      
    26         2,779          2,479           2,056          1,706          1,315          0,856          0,531          0,256          0,127      
    27         2,771          2,473           2,052          1,703          1,314          0,855          0,531          0,256          0,127      
    28         2,763          2,467           2,048          1,701          1,313          0,855           0,53          0,256          0,127      
    29         2,756          2,462           2,045          1,699          1,311          0,854           0,53          0,256          0,127      
    30         2,75           2,457           2,042          1,697           1,31          0,854           0,53          0,256          0,127      
    40         2,704          2,423           2,021          1,684          1,303          0,851          0,529          0,255          0,126      
    60         2,66            2,39             2            1,671          1,296          0,848          0,527          0,254          0,126      
    120        2,617          2,358           1,98           1,658          1,289          0,845          0,526          0,254          0,126      
   \>120       2.576          2.326           1.960          1.645          1.282          0.842          0.524          0.253          0.126      
  ------- --------------- -------------- --------------- -------------- -------------- -------------- -------------- -------------- -------------- --
:::
